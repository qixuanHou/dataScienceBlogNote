## Visual Analytics in Deep Learning:An Interrogative Survey for the Next Frontiers
### ABSTRACT
Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. 
a survey paper
### Introduction
![Alt Text]()
Deep learning is a specific set of techniques from the broader field of machine learning (ML) that focus on the study and usage of deep artificial neural networks to learn structured representations of data. First mentioned as early as the 1940s [1], artificial neural networks have a rich history [2], and have recently seen a dominate and pervasive resurgence [3], [4], [5] in many research domains by producing state-of-the-art results [6], [7] on a number of diverse big data tasks [8], [9]. For example, the premiere ma- chine learning, deep learning, and artificial intelligence (AI) conferences have seen enormous growth in attendance and paper submissions since early 2010s. Furthermore, open- source toolkits and programming libraries for building, training, and evaluating deep neural networks have become more robust and easy to use, democratizing deep learning. As a result, the barrier to developing deep learning models is lower than ever before and deep learning applications are
becoming pervasive.
While this technological progress is impressive, it comes
with unique and novel challenges. For example, the lack of interpretability and transparency of neural networks, from the learned representations to the underlying decision process, is an important problem to address. Making sense of why a particular model misclassifies test data instances or behaves poorly at times is a challenging task for model developers. Similarly, end-users interacting with an applica- tion that relies on deep learning to make critical decisions may question its reliability if no explanation is given by the model, or become baffled if the explanation is convoluted. While explaining neural network decisions is important, there are numerous other problems that arise from deep learning, such as AI safety and security (e.g., when using models in applications such as self-driving vehicles), and compromised trust due to bias in models and datasets, just to name a few. These challenges are often compounded, due to the large datasets required to train most deep learning models. As worrisome as these problems are, they will likely become even more widespread as more AI-powered systems are deployed in the world. Therefore, a general sense of model understanding is not only beneficial, but often required to address the aforementioned issues.
Data visualization and visual analytics excel at knowl- edge communication and insight discovery by using encod- ings to transform abstract data into meaningful represen- tations. In the seminal work by Zeiler and Fergus [10], a technique called deconvolutional networks enabled projection from a model’s learned feature space back to the pixel space. Their technique and results give insight into what types of features deep neural networks are learning at specific layers, and also serve as a debugging tool for improving a model. This work is often credited for popularizing vi- sualization in the machine learning and computer vision communities in recent years, putting a spotlight on it as a powerful tool that helps people understand and improve deep learning models. However, visualization research for neural networks started well before [11], [12], [13]. Over just a handful of years, many different techniques have been introduced to help interpret what neural networks are learning. Many such techniques generate static images, such as attention maps and heatmaps for image classification, indicating which parts of an image are most important to the classification. However, interaction has also been incorporated into the model understanding process in visual analytics tools to help people gain insight [14], [15], [16]. This hybrid research area has grown in both academia and industry, forming the basis for many new research papers, academic workshops, and deployed industry tools.
In this survey, we summarize a large number of deep learning visualization works using the Five W’s and How (Why, Who, What, How, When, and Where). Figure 1 presents a visual overview of how these interrogative ques- tions reveal and organize the various facets of deep learning visualization research and their related topics. By framing the survey in this way, many existing works fit a description as the following fictional example:
To interpret representations learned by deep models (why), model developers (who) visualize neuron activa- tions in convolutional neural networks (what) using t- SNE embeddings (how) after the training phase (when) to solve an urban planning problem (where).
This framing captures the needs, audience, and techniques of deep learning visualization, and positions new work’s contributions in the context of existing literature.
We conclude by highlighting prominent research direc- tions and open problems. We hope that this survey acts as a companion text for researchers and practitioners wishing to understand how visualization supports deep learning research and applications.




https://arxiv.org/pdf/1801.06889.pdf
